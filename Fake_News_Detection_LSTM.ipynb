{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemialisaaas/Fake-News-Detection/blob/main/Fake_News_Detection_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOpIdqa3naGv",
        "outputId": "7911c185-cbeb-4247-c210-03b86be24dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (75.1.0)\n",
            "Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.17.0 textstat-0.7.4\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Library**"
      ],
      "metadata": {
        "id": "8n53bhkX3wP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fake News Detection/Shuffle_50k_acak.csv')\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# Map labels to integers\n",
        "label_to_int = {\"Fake\": 0, \"Real\": 1}\n",
        "df['Label'] = df['Label'].map(label_to_int)\n",
        "print(\"Labels mapped successfully.\")\n",
        "\n",
        "# Split data into train and test\n",
        "X = df['stemmed_content']\n",
        "y = df['Label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences\n",
        "max_length = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Oversample without SMOTE\n",
        "train_data = pd.DataFrame(X_train_pad)\n",
        "train_data['label'] = y_train.values\n",
        "\n",
        "majority = train_data[train_data['label'] == 1]\n",
        "minority = train_data[train_data['label'] == 0]\n",
        "\n",
        "minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
        "balanced_data = pd.concat([majority, minority_upsampled])\n",
        "\n",
        "X_train_resampled = balanced_data.drop(columns=['label']).values\n",
        "y_train_resampled = balanced_data['label'].values\n",
        "\n",
        "# Display label distribution after oversampling\n",
        "print(\"Label distribution after oversampling:\")\n",
        "print(pd.Series(y_train_resampled).value_counts())\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=5000, output_dim=32, trainable=True))\n",
        "    model.add(SpatialDropout1D(0.6))\n",
        "    model.add(LSTM(units=8, kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01)))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-6)\n",
        "\n",
        "# Update training loop\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold = 1\n",
        "\n",
        "for train_idx, val_idx in kf.split(X_train_resampled):\n",
        "    print(f\"\\nTraining Fold {fold}...\")\n",
        "    X_train_fold, X_val_fold = X_train_resampled[train_idx], X_train_resampled[val_idx]\n",
        "    y_train_fold, y_val_fold = y_train_resampled[train_idx], y_train_resampled[val_idx]\n",
        "\n",
        "    model = create_model()\n",
        "    model.fit(\n",
        "        X_train_fold, y_train_fold,\n",
        "        validation_data=(X_val_fold, y_val_fold),\n",
        "        epochs=10,\n",
        "        batch_size=128,\n",
        "        callbacks=[early_stopping, lr_scheduler],\n",
        "        verbose=1\n",
        "    )\n",
        "    fold += 1\n",
        "\n",
        "# Final evaluation\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"\\nFinal Metrics:\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "\n",
        "# Save predictions\n",
        "prediction_df = pd.DataFrame({\n",
        "    'id_berita': df.iloc[y_test.index].index,\n",
        "    'actual_label': y_test.values,\n",
        "    'predicted_label': y_pred.flatten(),\n",
        "    'predicted_prob': y_pred_prob.flatten()\n",
        "})\n",
        "int_to_label = {0: 'Fake', 1: 'Real'}\n",
        "prediction_df['predicted_label'] = prediction_df['predicted_label'].map(int_to_label)\n",
        "prediction_df['actual_label'] = prediction_df['actual_label'].map(int_to_label)\n",
        "\n",
        "# Sort by predicted probability and display top 10 predictions\n",
        "top_predictions = prediction_df.sort_values(by='predicted_prob', ascending=False).head(10)\n",
        "print(\"\\nTop 10 Predictions:\")\n",
        "print(top_predictions)\n",
        "\n",
        "# Save all predictions and top 10 predictions\n",
        "prediction_df.to_csv('prediksi_berita_lstm_oversample.csv', index=False)\n",
        "top_predictions.to_csv('top_10_predictions.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'prediksi_berita_lstm_oversample.csv'.\")\n",
        "print(\"Top 10 predictions saved to 'top_10_predictions.csv'.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlF3RM3LzGo2",
        "outputId": "ecc17af3-630e-4525-ebba-e8b4f92a27b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Labels mapped successfully.\n",
            "Label distribution after oversampling:\n",
            "1    26480\n",
            "0    26480\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training Fold 1...\n",
            "Epoch 1/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.7711 - loss: 0.7453 - val_accuracy: 0.9944 - val_loss: 0.1512 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.9794 - loss: 0.1834 - val_accuracy: 0.9945 - val_loss: 0.0941 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.9743 - loss: 0.1845 - val_accuracy: 0.9951 - val_loss: 0.0874 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.9635 - loss: 0.2158 - val_accuracy: 0.9950 - val_loss: 0.1026 - learning_rate: 0.0010\n",
            "\n",
            "Training Fold 2...\n",
            "Epoch 1/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.7717 - loss: 0.7256 - val_accuracy: 0.9968 - val_loss: 0.1625 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.9367 - loss: 0.3113 - val_accuracy: 0.9932 - val_loss: 0.1585 - learning_rate: 0.0010\n",
            "\n",
            "Training Fold 3...\n",
            "Epoch 1/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.7609 - loss: 0.7506 - val_accuracy: 0.9902 - val_loss: 0.1800 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9748 - loss: 0.2120 - val_accuracy: 0.8510 - val_loss: 0.4632 - learning_rate: 0.0010\n",
            "\n",
            "Training Fold 4...\n",
            "Epoch 1/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.7528 - loss: 0.7473 - val_accuracy: 0.9872 - val_loss: 0.2027 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9786 - loss: 0.2154 - val_accuracy: 0.9918 - val_loss: 0.1206 - learning_rate: 0.0010\n",
            "\n",
            "Training Fold 5...\n",
            "Epoch 1/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.7598 - loss: 0.7455 - val_accuracy: 0.9954 - val_loss: 0.1875 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9815 - loss: 0.2179 - val_accuracy: 0.9927 - val_loss: 0.1137 - learning_rate: 0.0010\n",
            "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "\n",
            "Final Metrics:\n",
            "Precision: 0.9965267290848686\n",
            "Recall: 0.9968277945619335\n",
            "F1-score: 0.9966772390877511\n",
            "Accuracy: 0.9956310197597061\n",
            "Confusion Matrix:\n",
            "[[3428   23]\n",
            " [  21 6599]]\n",
            "\n",
            "Top 10 Predictions:\n",
            "      id_berita actual_label predicted_label  predicted_prob\n",
            "5435      18061         Real            Real        0.915990\n",
            "1459       1667         Real            Real        0.915988\n",
            "6200      43212         Real            Real        0.915987\n",
            "4058      31896         Real            Real        0.915984\n",
            "875       31524         Real            Real        0.915979\n",
            "9705      24662         Real            Real        0.915977\n",
            "3090       5242         Real            Real        0.915976\n",
            "8530      21363         Real            Real        0.915976\n",
            "8331      36955         Real            Real        0.915975\n",
            "2002      31914         Real            Real        0.915974\n",
            "Predictions saved to 'prediksi_berita_lstm_oversample.csv'.\n",
            "Top 10 predictions saved to 'top_10_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScHvkEl-mrAS",
        "outputId": "69f27454-ff95-446b-fbd9-5b9a2487aa37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Labels mapped successfully.\n",
            "Train size: 32730, Validation size: 7554, Test size: 10071\n",
            "Label distribution after oversampling:\n",
            "1    21515\n",
            "0    21515\n",
            "Name: count, dtype: int64\n",
            "Epoch 1/20\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 86ms/step - accuracy: 0.8876 - loss: 0.8990 - val_accuracy: 0.9011 - val_loss: 0.3691 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 75ms/step - accuracy: 0.9262 - loss: 0.2805 - val_accuracy: 0.9697 - val_loss: 0.1675 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 72ms/step - accuracy: 0.9736 - loss: 0.1622 - val_accuracy: 0.9673 - val_loss: 0.1721 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 72ms/step - accuracy: 0.9710 - loss: 0.1656 - val_accuracy: 0.9611 - val_loss: 0.1813 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 72ms/step - accuracy: 0.9664 - loss: 0.1738 - val_accuracy: 0.9588 - val_loss: 0.1840 - learning_rate: 5.0000e-04\n",
            "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step\n",
            "\n",
            "Final Metrics:\n",
            "Precision: 0.9928637914986038\n",
            "Recall: 0.9667673716012085\n",
            "F1-score: 0.9796418184601255\n",
            "Accuracy: 0.9735875285473141\n",
            "Confusion Matrix:\n",
            "[[3405   46]\n",
            " [ 220 6400]]\n",
            "\n",
            "Top 10 Predictions:\n",
            "      id_berita actual_label predicted_label  predicted_prob\n",
            "1319      48178         Real            Real        0.973424\n",
            "8816      49001         Real            Real        0.973423\n",
            "7598      21363         Real            Real        0.973423\n",
            "8892      46648         Real            Real        0.973423\n",
            "2498      29448         Real            Real        0.973423\n",
            "8330      43212         Real            Real        0.973422\n",
            "364        3455         Real            Real        0.973422\n",
            "5043      12468         Real            Real        0.973422\n",
            "4370      17342         Real            Real        0.973422\n",
            "8280      12232         Real            Real        0.973422\n",
            "Predictions saved to 'prediksi_berita_lstm_oversample.csv'.\n",
            "Top 10 predictions saved to 'top_10_predictions.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fake News Detection/Shuffle_50k_acak.csv')\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# Map labels to integers\n",
        "label_to_int = {\"Fake\": 0, \"Real\": 1}\n",
        "df['Label'] = df['Label'].map(label_to_int)\n",
        "print(\"Labels mapped successfully.\")\n",
        "\n",
        "# Split data into train (65%), validation (15%), and test (20%)\n",
        "X = df['stemmed_content']\n",
        "y = df['Label']\n",
        "\n",
        "# Step 1: Split into train (65%) and temp (35%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.35, stratify=y, random_state=42)\n",
        "\n",
        "# Step 2: Split temp into validation (15%) and test (20%)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5714, stratify=y_temp, random_state=42)  # 0.5714 = 20% / 35%\n",
        "\n",
        "print(f\"Train size: {len(X_train)}, Validation size: {len(X_valid)}, Test size: {len(X_test)}\")\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_valid_seq = tokenizer.texts_to_sequences(X_valid)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences\n",
        "max_length = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "X_valid_pad = pad_sequences(X_valid_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Oversample without SMOTE\n",
        "train_data = pd.DataFrame(X_train_pad)\n",
        "train_data['label'] = y_train.values\n",
        "\n",
        "majority = train_data[train_data['label'] == 1]\n",
        "minority = train_data[train_data['label'] == 0]\n",
        "\n",
        "minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
        "balanced_data = pd.concat([majority, minority_upsampled])\n",
        "\n",
        "X_train_resampled = balanced_data.drop(columns=['label']).values\n",
        "y_train_resampled = balanced_data['label'].values\n",
        "\n",
        "# Display label distribution after oversampling\n",
        "print(\"Label distribution after oversampling:\")\n",
        "print(pd.Series(y_train_resampled).value_counts())\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=5000, output_dim=64, trainable=True))\n",
        "    model.add(SpatialDropout1D(0.4))  # Dropout untuk mengurangi overfitting\n",
        "    model.add(LSTM(units=32, kernel_regularizer=l2(0.01), return_sequences=True))\n",
        "    model.add(Dropout(0.4))  # Dropout tambahan setelah LSTM\n",
        "    model.add(LSTM(units=16, kernel_regularizer=l2(0.01)))  # LSTM tambahan untuk representasi lebih dalam\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01)))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
        "\n",
        "# Train the model\n",
        "model = create_model()\n",
        "history = model.fit(\n",
        "    X_train_resampled, y_train_resampled,\n",
        "    validation_data=(X_valid_pad, y_valid),\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"\\nFinal Metrics:\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Save predictions\n",
        "prediction_df = pd.DataFrame({\n",
        "    'id_berita': df.iloc[y_test.index].index,\n",
        "    'actual_label': y_test.values,\n",
        "    'predicted_label': y_pred.flatten(),\n",
        "    'predicted_prob': y_pred_prob.flatten()\n",
        "})\n",
        "int_to_label = {0: 'Fake', 1: 'Real'}\n",
        "prediction_df['predicted_label'] = prediction_df['predicted_label'].map(int_to_label)\n",
        "prediction_df['actual_label'] = prediction_df['actual_label'].map(int_to_label)\n",
        "\n",
        "# Sort by predicted probability and display top 10 predictions\n",
        "top_predictions = prediction_df.sort_values(by='predicted_prob', ascending=False).head(10)\n",
        "print(\"\\nTop 10 Predictions:\")\n",
        "print(top_predictions)\n",
        "\n",
        "# Save all predictions and top 10 predictions\n",
        "prediction_df.to_csv('prediksi_berita_lstm_oversample.csv', index=False)\n",
        "top_predictions.to_csv('top_10_predictions.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'prediksi_berita_lstm_oversample.csv'.\")\n",
        "print(\"Top 10 predictions saved to 'top_10_predictions.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fake News Detection/Shuffle_50k_acak.csv')\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# Map labels to integers\n",
        "label_to_int = {\"Fake\": 0, \"Real\": 1}\n",
        "df['Label'] = df['Label'].map(label_to_int)\n",
        "print(\"Labels mapped successfully.\")\n",
        "\n",
        "# Split data into train (65%), validation (15%), and test (20%)\n",
        "X = df['stemmed_content']\n",
        "y = df['Label']\n",
        "\n",
        "# Step 1: Split into train (65%) and temp (35%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.35, stratify=y, random_state=42)\n",
        "\n",
        "# Step 2: Split temp into validation (15%) and test (20%)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5714, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(f\"Train size: {len(X_train)}, Validation size: {len(X_valid)}, Test size: {len(X_test)}\")\n",
        "\n",
        "# Preprocess text for Word2Vec\n",
        "def preprocess_text(text):\n",
        "    return text.lower().split()  # Split text into words (you can add more preprocessing here)\n",
        "\n",
        "X_train_preprocessed = X_train.apply(preprocess_text)\n",
        "X_valid_preprocessed = X_valid.apply(preprocess_text)\n",
        "X_test_preprocessed = X_test.apply(preprocess_text)\n",
        "\n",
        "# Train Word2Vec model\n",
        "print(\"Training Word2Vec model...\")\n",
        "word2vec_model = Word2Vec(sentences=X_train_preprocessed, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print(\"Word2Vec model trained.\")\n",
        "\n",
        "# Build embedding matrix\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train_preprocessed)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "embedding_dim = 100  # Same as Word2Vec vector size\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_preprocessed)\n",
        "X_valid_seq = tokenizer.texts_to_sequences(X_valid_preprocessed)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_preprocessed)\n",
        "\n",
        "max_length = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "X_valid_pad = pad_sequences(X_valid_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Define LSTM model using Word2Vec embedding\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_length,\n",
        "                        trainable=False))  # Freeze the embedding layer\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(LSTM(units=32, return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(units=16))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
        "\n",
        "# Train the model\n",
        "model = create_model()\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_valid_pad, y_valid),\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"\\nFinal Metrics:\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Save predictions\n",
        "prediction_df = pd.DataFrame({\n",
        "    'id_berita': df.iloc[y_test.index].index,\n",
        "    'actual_label': y_test.values,\n",
        "    'predicted_label': y_pred.flatten(),\n",
        "    'predicted_prob': y_pred_prob.flatten()\n",
        "})\n",
        "int_to_label = {0: 'Fake', 1: 'Real'}\n",
        "prediction_df['predicted_label'] = prediction_df['predicted_label'].map(int_to_label)\n",
        "prediction_df['actual_label'] = prediction_df['actual_label'].map(int_to_label)\n",
        "\n",
        "# Sort by predicted probability and display top 10 predictions\n",
        "top_predictions = prediction_df.sort_values(by='predicted_prob', ascending=False).head(10)\n",
        "print(\"\\nTop 10 Predictions:\")\n",
        "print(top_predictions)\n",
        "\n",
        "# Save all predictions and top 10 predictions\n",
        "prediction_df.to_csv('prediksi_berita_word2vec_lstm.csv', index=False)\n",
        "top_predictions.to_csv('top_10_predictions_word2vec_lstm.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'prediksi_berita_word2vec_lstm.csv'.\")\n",
        "print(\"Top 10 predictions saved to 'top_10_predictions_word2vec_lstm.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPtfhhRmrWKA",
        "outputId": "518dd283-9323-4319-a4a1-ce23f4201860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Labels mapped successfully.\n",
            "Train size: 32730, Validation size: 7554, Test size: 10071\n",
            "Training Word2Vec model...\n",
            "Word2Vec model trained.\n",
            "Embedding matrix shape: (68136, 100)\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - accuracy: 0.8911 - loss: 0.2345 - val_accuracy: 0.9950 - val_loss: 0.0292 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - accuracy: 0.9904 - loss: 0.0442 - val_accuracy: 0.9981 - val_loss: 0.0125 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 60ms/step - accuracy: 0.9947 - loss: 0.0245 - val_accuracy: 0.9995 - val_loss: 0.0052 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - accuracy: 0.9952 - loss: 0.0202 - val_accuracy: 0.9999 - val_loss: 0.0022 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - accuracy: 0.9982 - loss: 0.0083 - val_accuracy: 0.9999 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - accuracy: 0.9988 - loss: 0.0050 - val_accuracy: 0.9996 - val_loss: 0.0027 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - accuracy: 0.9986 - loss: 0.0046 - val_accuracy: 0.9997 - val_loss: 6.3835e-04 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 60ms/step - accuracy: 0.9990 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 3.6099e-04 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 0.9989 - val_loss: 0.0049 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - accuracy: 0.9987 - loss: 0.0053 - val_accuracy: 0.9992 - val_loss: 0.0056 - learning_rate: 0.0010\n",
            "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step\n",
            "\n",
            "Final Metrics:\n",
            "Precision: 1.0\n",
            "Recall: 0.9990936555891239\n",
            "F1-score: 0.9995466223364062\n",
            "Accuracy: 0.9994042299672327\n",
            "Confusion Matrix:\n",
            "[[3451    0]\n",
            " [   6 6614]]\n",
            "\n",
            "Top 10 Predictions:\n",
            "      id_berita actual_label predicted_label  predicted_prob\n",
            "6875      26757         Real            Real        0.999924\n",
            "3426      34879         Real            Real        0.999924\n",
            "990       25763         Real            Real        0.999924\n",
            "1209      29414         Real            Real        0.999924\n",
            "5980       5404         Real            Real        0.999924\n",
            "6997      13866         Real            Real        0.999924\n",
            "5873      18673         Real            Real        0.999924\n",
            "4269      18826         Real            Real        0.999924\n",
            "4797      44289         Real            Real        0.999924\n",
            "4367       1513         Real            Real        0.999924\n",
            "Predictions saved to 'prediksi_berita_word2vec_lstm.csv'.\n",
            "Top 10 predictions saved to 'top_10_predictions_word2vec_lstm.csv'.\n"
          ]
        }
      ]
    }
  ]
}